{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e4af0c-18f3-43f4-b418-17b48f156ef8",
   "metadata": {},
   "source": [
    "# Trendspotting POC\n",
    "\n",
    "Goal of this notebook is to\n",
    "Load signals data into a managed vertex dataset for time series forecasting\n",
    "Create a forecast prediction model for each term, geo and category combination\n",
    "Project the forecasts on a holdout set of data to assess performance and trends\n",
    "\n",
    "When run - the piepline will look something like this:\n",
    "\n",
    "![pipeline example](img/pipeline_example.png)\n",
    "\n",
    "Todo: Integration of ingredients, flagging of trends\n",
    "\n",
    "## Install packages, create bucket (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5999fd54-5659-406e-87f5-b0778db565ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # New\n",
    "# ! pip3 install -U google-cloud-storage --user\n",
    "# # ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "# !git clone https://github.com/kubeflow/pipelines.git\n",
    "# !pip install pipelines/components/google-cloud/.\n",
    "# !pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4daa105-58de-4046-be85-779f845936e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l us-central1 gs://trendspotting-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15945c-b978-45c4-9ecc-87e17c4097b6",
   "metadata": {},
   "source": [
    "### Import libs and types for KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00b2a410-def8-43de-aa41-2ed9efe75d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from google import auth\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "import google.cloud.aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a95fbc67-a62b-42f2-a756-a6e304bf582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'cgp-cdp'\n",
    "LOCATION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b675de8f-00f9-4d2f-9d5c-73e3d9e78683",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = 'gs://trendspotting-pipeline' # <--- TODO: CHANGE THIS; can be blank json file\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "  with open(PIPELINES_FILEPATH) as f:\n",
    "    PIPELINES = json.load(f)\n",
    "else:\n",
    "  PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "  with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "    json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d66fd7-22b7-4682-ad27-dfdb3bc1c551",
   "metadata": {},
   "source": [
    "### KFP Custom Component - training data query\n",
    "\n",
    "Details: From `futurama_weekly` pull data between 7/20 - 12/21 (100 gb limit for automl tables). Automatically set testing and validation as follows:\n",
    "    \n",
    "* Train: 2/20-4/21\n",
    "* Validate: 5/21-6/21\n",
    "* Test: 6/21-12/21\n",
    "    \n",
    "Also set `series_id` to be a concat: `concat(category_id, geo_id, term) as series_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff31cb65-ec36-44f3-963c-e9a8dc970a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==2.18.0'],\n",
    ")\n",
    "def create_prediction_dataset(\n",
    "  project: str,\n",
    "  dataset: str,\n",
    "  source_table_uri: str,\n",
    "  override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [('training_data_table_uri', str)]):\n",
    "  from google.cloud import bigquery\n",
    " \n",
    "  override = bool(override)\n",
    "  bq_client = bigquery.Client(project=project)\n",
    "  combined_preds_forecast_table_name = f'{project}.{dataset}.y5y6_forecast_volume_term'\n",
    "  (\n",
    "    bq_client.query(\n",
    "      f\"\"\"\n",
    "     create table if not exists `{combined_preds_forecast_table_name}` as (\n",
    "         with raw_data as (\n",
    "         SELECT *, concat(category_id, geo_id, term) as series_id,\n",
    "         case when date between '2020-02-01' and  '2021-04-01' then 'TRAIN'\n",
    "          when date between '2021-05-01' and '2021-05-31' then 'VALIDATE'\n",
    "         else 'TEST' end as split_col\n",
    "         from `cpg-cdp.trendspotting.futurama_weekly`\n",
    "         WHERE date between '2020-07-01' and '2021-12-31'\n",
    "         )\n",
    "         SELECT * EXCEPT (volume, score, geo_type) from raw_data\n",
    ")\n",
    "          \"\"\"\n",
    "    )\n",
    "    .result()\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    f'bq://{combined_preds_forecast_table_name}',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca3afee2-470b-445d-8360-1cbf89b3d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "VERSION = 'poc'\n",
    "rmse_model_version = 'poc'\n",
    "\n",
    "COLUMN_TRANSFORMATIONS = [\n",
    "  {\n",
    "    \"timestamp\": {\n",
    "      \"columnName\": \"date\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"categorical\": {\n",
    "      \"columnName\": \"geo_id\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"text\": {\n",
    "      \"columnName\": \"geo_name\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"categorical\": {\n",
    "      \"columnName\": \"category_id\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"text\": {\n",
    "      \"columnName\": \"term\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"numeric\": {\n",
    "      \"columnName\": \"category_rank\"\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cfd3a-e4b3-4de6-9486-901b6ab7583f",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "Uses custom components, also uses reusable vertex components for creating the training dataset and training the forecast models\n",
    "\n",
    "Notice the output for testing in BQ is set by `target_table`, assigned to `export_evaluated_data_items_bigquery_destination_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61b9d896-c952-40b1-ba3b-4b14a973fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_TAG = 'trendspotting-pipeline' # <--- TODO; optionally name pipeline\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{VERSION}-{PIPELINE_TAG}'.replace('_', '-'),\n",
    "        pipeline_root=PIPELINES_FILEPATH,\n",
    "\n",
    ")\n",
    "def pipeline(\n",
    "  vertex_project: str,\n",
    "  location: str,\n",
    "  version: str,\n",
    "  data_source_dataset: str,\n",
    "  ds_display_name: str,\n",
    "  # activities_expected_historical_last_date: str,\n",
    "  context_window: int,\n",
    "  forecast_horizon: int,\n",
    "  override: str,\n",
    "  target_table: str,\n",
    "  budget_milli_node_hours: int = 16000,\n",
    "):\n",
    "\n",
    "    \n",
    "  create_prediction_dataset_op = create_prediction_dataset(\n",
    "  project = vertex_project,\n",
    "  dataset = 'trendspotting',\n",
    "  source_table_uri = data_source_dataset,\n",
    "  override = 'False',)\n",
    "\n",
    "\n",
    "  time_series_dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "    display_name=ds_display_name, \n",
    "    bq_source=create_prediction_dataset_op.outputs['training_data_table_uri'],\n",
    "    project=vertex_project,\n",
    "    location=location,\n",
    "  )\n",
    "  rmse_model_op = gcc_aip_forecasting.ForecastingTrainingWithExperimentsOp(\n",
    "      display_name=f'train-{rmse_model_version}',\n",
    "      model_display_name=rmse_model_version,\n",
    "      dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "      context_window=context_window,\n",
    "      forecast_horizon=forecast_horizon,\n",
    "      budget_milli_node_hours=budget_milli_node_hours,\n",
    "      project=vertex_project,\n",
    "      location=location,\n",
    "      export_evaluated_data_items=True,\n",
    "      export_evaluated_data_items_override_destination=True,\n",
    "      target_column='category_rank',\n",
    "      time_column='date',\n",
    "      time_series_identifier_column='series_id',\n",
    "      time_series_attribute_columns=['geo_name', 'geo_id', 'category_id', 'term'],\n",
    "      unavailable_at_forecast_columns=['category_rank'],\n",
    "      available_at_forecast_columns=['date'],\n",
    "      data_granularity_unit='week',\n",
    "      data_granularity_count=1,\n",
    "      predefined_split_column_name= 'split_col', \n",
    "      optimization_objective='minimize-rmse',\n",
    "      column_transformations=COLUMN_TRANSFORMATIONS,\n",
    "      export_evaluated_data_items_bigquery_destination_uri = target_table, # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8d9ec46-05d3-4dba-a935-c58026a5e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='trendspotting.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f6a37-e374-4570-b462-fd9d6b93691f",
   "metadata": {},
   "source": [
    "### Set parameters for pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a112290-eb83-48b6-8006-fdd244a02679",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'cpg-cdp' # <--- TODO: If not set\n",
    "LOCATION = 'us-central1' # <--- TODO: If not set\n",
    "SERVICE_ACCOUNT = 'vertex-pipelines@cpg-cdp.iam.gserviceaccount.com' , # <--- TODO: Change This if needed\n",
    "\n",
    "# BQ dataset for source data source\n",
    "DATA_SOURCE_DATASET = 'futurama_weekly'\n",
    "\n",
    "# TODO: Forecasting Configuration:\n",
    "HISTORY_WINDOW_n = 52 #  {type: 'integer'} # context_window\n",
    "FORECAST_HORIZON = 52 #  {type: 'integer'} \n",
    "BUDGET_MILLI_NODE_HOURS = 16000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bc9f7ac-9937-4f1a-baa4-643d6e688648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b8ed7-5b52-4869-9ad0-5ec0981bb996",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the pipeline\n",
    "Follow the link to see the exectution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47ad4b6c-5843-4f36-ba64-3b1869c630e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/939655404703/locations/us-central1/pipelineJobs/poc-trendspotting-pipeline-20220208203238\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/939655404703/locations/us-central1/pipelineJobs/poc-trendspotting-pipeline-20220208203238')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/poc-trendspotting-pipeline-20220208203238?project=939655404703\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "PIPELINE_PARAMETERS = {\n",
    "      'vertex_project': PROJECT_ID,\n",
    "      'location': LOCATION,\n",
    "      'version': VERSION,\n",
    "      'data_source_dataset': DATA_SOURCE_DATASET,\n",
    "      'context_window': HISTORY_WINDOW_n,\n",
    "      'forecast_horizon': FORECAST_HORIZON,\n",
    "      'budget_milli_node_hours': BUDGET_MILLI_NODE_HOURS,\n",
    "      'ds_display_name': '20-21-clean',\n",
    "      'override' : 'false',\n",
    "      'target_table' : f'bq://{PROJECT_ID}:trendspotting.predict_c{HISTORY_WINDOW_n}_h{FORECAST_HORIZON}'\n",
    "    }\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = 'trendspotting_test',\n",
    "                             template_path = 'trendspotting.json',\n",
    "                             pipeline_root = PIPELINES_FILEPATH,\n",
    "                             parameter_values = PIPELINE_PARAMETERS,\n",
    "                             project = PROJECT_ID,\n",
    "                             location = LOCATION)\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f58f3f-8715-4947-bf4d-6fac75f560f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #         note have to clean the output a bit for casting\n",
    "    \n",
    "#     create table `cpg-cdp.trendspotting.predict_c52_h52_fixed` as\n",
    "#  SELECT *, cast(category_rank as int) as category_rank_int  FROM `cpg-cdp.trendspotting.predict_c52_h52`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae2721-0074-4077-aa12-0d9859b081a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673899f3-3138-40ed-9334-0b777a412c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
