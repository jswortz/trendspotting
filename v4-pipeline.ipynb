{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e4af0c-18f3-43f4-b418-17b48f156ef8",
   "metadata": {},
   "source": [
    "# Trendspotting POC\n",
    "\n",
    "Goal of this notebook is to\n",
    "* Load signals data into a managed vertex dataset for time series forecasting\n",
    "* Create a forecast prediction model for each term, geo and category combination\n",
    "* Project the forecasts on a holdout set of data to assess performance and trends\n",
    "* Clean up results of test predictions\n",
    "* Cluster test predictions\n",
    "* Create dashboard for backtesting\n",
    "\n",
    "End users would take this parameterized pipeline to produce futurama backtests using clustering and forecasting\n",
    "\n",
    "[Source Control Link](https://source.cloud.google.com/cpg-cdp/trendspotting/+/master:pipeline_train.ipynb)\n",
    "\n",
    "When run - the piepline will look something like this:\n",
    "\n",
    "![pipeline example](img/pipeline_example.png)\n",
    "\n",
    "[Link to pipeline](https://pantheon.corp.google.com/vertex-ai/locations/us-central1/pipelines/runs/report-pipe-trendspotting-pipeline-20220309212043?authuser=0&project=cpg-cdp)\n",
    "## Install packages, create bucket (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d303d30-5821-4179-a13d-3c49455e72ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components==1.0.5\n",
    "# # !git clone https://github.com/kubeflow/pipelines.git\n",
    "# # !pip install pipelines/components/google-cloud/.\n",
    "# !pip install google-cloud-aiplatform\n",
    "# from google_cloud_pipeline_components.v1.bigquery import BigqueryCreateModelJobOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15945c-b978-45c4-9ecc-87e17c4097b6",
   "metadata": {},
   "source": [
    "### Import libs and types for KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b2a410-def8-43de-aa41-2ed9efe75d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "from IPython.display import clear_output\n",
    "from google_cloud_pipeline_components.v1.bigquery import BigqueryCreateModelJobOp\n",
    "\n",
    "from google import auth\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "from google_cloud_pipeline_components.experimental import bigquery as gcp_aip_bq\n",
    "\n",
    "import google.cloud.aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Input, Model\n",
    "from kfp.v2.components.types.type_utils import artifact_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95fbc67-a62b-42f2-a756-a6e304bf582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'cpg-cdp'\n",
    "LOCATION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b675de8f-00f9-4d2f-9d5c-73e3d9e78683",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = 'gs://trendspotting-pipeline' # <--- TODO: CHANGE THIS; can be blank json file\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "    with open(PIPELINES_FILEPATH) as f:\n",
    "        PIPELINES = json.load(f)\n",
    "else:\n",
    "    PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "    with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "        json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d66fd7-22b7-4682-ad27-dfdb3bc1c551",
   "metadata": {},
   "source": [
    "### KFP Custom Component - training data query\n",
    "\n",
    "Details: From `futurama_weekly` pull data between 7/20 - 12/21 (100 gb limit for automl tables). Automatically set testing and validation as follows:\n",
    "    \n",
    "* Train: 2/20-4/21\n",
    "* Validate: 5/21-6/21\n",
    "* Test: 6/21-12/21\n",
    "    \n",
    "Also set `series_id` to be a concat: `concat(category_id, geo_id, term) as series_id`\n",
    "\n",
    "Note this will create a backtest for the test period to understand the effecicacy of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46e5f2-b27f-4286-98e2-83f11962f217",
   "metadata": {},
   "source": [
    "### Starter component - check source table date for run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f1d97-61ad-4a31-92ba-88428dd4a61b",
   "metadata": {},
   "source": [
    "#### Detailed Report Custom Component\n",
    "\n",
    "This creates the futurama weekly table with Swivel NLP embeddings (20 float fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a6224-65e4-40d1-bc98-06995d1d8a05",
   "metadata": {},
   "source": [
    "## Top level report\n",
    "\n",
    "This component takes futurama weekly, adds the NLP features plus applies the trained 100 cluster model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853aea81-7902-4102-88f6-098a198036e8",
   "metadata": {},
   "source": [
    "#### Creation of aggregated cluster-level data\n",
    "Used in the cluster forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6138f3d-7d16-46c5-aa34-ef66bc40aac9",
   "metadata": {},
   "source": [
    "### Feature Specs for forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cfd3a-e4b3-4de6-9486-901b6ab7583f",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "Uses custom components, also uses reusable vertex components for creating the training dataset and training the forecast models\n",
    "\n",
    "Notice the output for testing in BQ is set by `target_table`, assigned to `export_evaluated_data_items_bigquery_destination_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec99f8a-ec3a-45d9-89c9-7f24e8d8f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components import components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b9d896-c952-40b1-ba3b-4b14a973fcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERSION = 'v4'\n",
    "SUFFIX = \"png_hair_22\"\n",
    "PIPELINE_TAG = f'{SUFFIX}-trendspotting-pipeline-{VERSION}' # <--- TODO; optionally name pipeline\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{VERSION}-{PIPELINE_TAG}'.replace('_', '-'),\n",
    "        pipeline_root=PIPELINES_FILEPATH,\n",
    "\n",
    ")\n",
    "def pipeline(\n",
    "    vertex_project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    ds_display_name_terms: str,\n",
    "    ds_display_name_cluster: str,\n",
    "    label_table: str,\n",
    "    scored_classification_table: str,\n",
    "    classification_train_table: str,\n",
    "    classification_model_name: str,\n",
    "    classification_model_budget: int,\n",
    "    auto_cluster_train_table: str,\n",
    "    auto_min_cluster: int,\n",
    "    auto_max_cluster: int,\n",
    "    auto_cluster_target_table: str,\n",
    "    label_list: list,\n",
    "    train_st: str,\n",
    "    train_end: str,\n",
    "    valid_st: str,\n",
    "    valid_end: str,\n",
    "    predict_on_dt: str,\n",
    "    override: str,\n",
    "    fix_embed_target: str,\n",
    "    k_means_name: str,\n",
    "    n_clusters: int,\n",
    "    top_n_results: int,\n",
    "    six_month_dt: str,\n",
    "    source_table: str ,\n",
    "    target_term_forecast_table: str ,\n",
    "    target_cluster_forecast_table: str ,\n",
    "    budget_milli_node_hours: int,\n",
    "    budget_milli_node_hours_cluster: int,\n",
    "    context_window: int,\n",
    "    forecast_horizon: int,\n",
    "    top_movers_target_table: str,\n",
    "    cluster_table_agg: str,\n",
    "    cluster_table: str,\n",
    "    subcat_id: int,\n",
    "    model_name: str,\n",
    "):\n",
    "\n",
    "\n",
    "    # get_data_source = gcp_aip_bq.BigqueryQueryJobOp(\n",
    "    #   project = 'cpg-cdp',\n",
    "    #   location = 'US',\n",
    "    #   query = f\"\"\"select distinct date from {source_table}\"\"\",\n",
    "    #     # encryption_spec_key_name=CMEK\n",
    "    # )\n",
    "    \n",
    "    embed_terms = components.create_prediction_dataset_term_level(\n",
    "      target_table = f'cpg-cdp.trendspotting.ETL_futurama_weekly_embed_{SUFFIX}',\n",
    "      source_table_uri = source_table,\n",
    "      train_st = train_st,\n",
    "      train_end = train_end,\n",
    "      valid_st = valid_st,\n",
    "      valid_end = valid_end,\n",
    "      subcat_id = subcat_id,\n",
    "    ) #-> NamedTuple('Outputs', [('training_data_table_uri', str)])j\n",
    "    \n",
    "    fix_embed = components.prep_forecast_term_level(\n",
    "        source_table = embed_terms.outputs['training_data_table_uri'],\n",
    "        target_table = fix_embed_target,\n",
    "        )# -> NamedTuple('Outputs', [('term_train_table', str)]):\n",
    "\n",
    "\n",
    "    time_series_dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "        display_name=ds_display_name_terms, \n",
    "        bq_source=fix_embed.outputs['term_train_table'],\n",
    "        project=vertex_project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    term_forecasting_op = gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "        display_name=f'train-point-forecast-futurama',\n",
    "        model_display_name='point-forecast-futurama',\n",
    "        dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "        context_window=context_window,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        budget_milli_node_hours=budget_milli_node_hours,\n",
    "        project=vertex_project,\n",
    "        location=location,\n",
    "        export_evaluated_data_items=True,\n",
    "        export_evaluated_data_items_override_destination=True,\n",
    "        target_column='category_rank',\n",
    "        time_column='date',\n",
    "        time_series_identifier_column='series_id',\n",
    "        time_series_attribute_columns=['geo_name', 'geo_id', 'category_id', 'term', \n",
    "                                      'emb1', 'emb2', 'emb3', 'emb4', 'emb5', 'emb6',\n",
    "                                      'emb7', 'emb8', 'emb9', 'emb10', 'emb11', 'emb12',\n",
    "                                      'emb13', 'emb14', 'emb15', 'emb16', 'emb17', 'emb18', \n",
    "                                      'emb19', 'emb20', 'sentences'],\n",
    "        unavailable_at_forecast_columns=['category_rank'],\n",
    "        available_at_forecast_columns=['date'],\n",
    "        data_granularity_unit='week',\n",
    "        data_granularity_count=1,\n",
    "        predefined_split_column_name= 'split_col', \n",
    "        optimization_objective='minimize-rmse',\n",
    "        column_transformations=components.COLUMN_TRANSFORMATIONS,\n",
    "        export_evaluated_data_items_bigquery_destination_uri = target_term_forecast_table, # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "    )\n",
    "    \n",
    "    top_movers_data_op = components.create_top_mover_table(source_table = target_term_forecast_table,\n",
    "    target_table = top_movers_target_table,\n",
    "        predict_on_dt = predict_on_dt, \n",
    "        six_month_dt = six_month_dt,\n",
    "        trained_model = term_forecasting_op.outputs['model'],\n",
    "        top_n_results = top_n_results,\n",
    "        ) #-> NamedTuple('Outputs', [('term_train_table', str)]):\n",
    "    \n",
    "    \n",
    "    #HIGH LEVEL REPORT PIPELINE STARTS HERE\n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    model_train_sql = f\"\"\"CREATE TEMPORARY FUNCTION arr_to_input_20(arr ARRAY<FLOAT64>)\n",
    "                        RETURNS \n",
    "                        STRUCT<p1 FLOAT64, p2 FLOAT64, p3 FLOAT64, p4 FLOAT64,\n",
    "                               p5 FLOAT64, p6 FLOAT64, p7 FLOAT64, p8 FLOAT64, \n",
    "                               p9 FLOAT64, p10 FLOAT64, p11 FLOAT64, p12 FLOAT64, \n",
    "                               p13 FLOAT64, p14 FLOAT64, p15 FLOAT64, p16 FLOAT64,\n",
    "                               p17 FLOAT64, p18 FLOAT64, p19 FLOAT64, p20 FLOAT64>\n",
    "                        AS (\n",
    "                        STRUCT(\n",
    "                            arr[OFFSET(0)]\n",
    "                            , arr[OFFSET(1)]\n",
    "                            , arr[OFFSET(2)]\n",
    "                            , arr[OFFSET(3)]\n",
    "                            , arr[OFFSET(4)]\n",
    "                            , arr[OFFSET(5)]\n",
    "                            , arr[OFFSET(6)]\n",
    "                            , arr[OFFSET(7)]\n",
    "                            , arr[OFFSET(8)]\n",
    "                            , arr[OFFSET(9)]\n",
    "                            , arr[OFFSET(10)]\n",
    "                            , arr[OFFSET(11)]\n",
    "                            , arr[OFFSET(12)]\n",
    "                            , arr[OFFSET(13)]\n",
    "                            , arr[OFFSET(14)]\n",
    "                            , arr[OFFSET(15)]\n",
    "                            , arr[OFFSET(16)]\n",
    "                            , arr[OFFSET(17)]\n",
    "                            , arr[OFFSET(18)]\n",
    "                            , arr[OFFSET(19)]    \n",
    "                        ));\n",
    "                        \n",
    "            CREATE OR REPLACE MODEL `{model_name}` OPTIONS(model_type='kmeans', KMEANS_INIT_METHOD='KMEANS++', num_clusters={n_clusters}) AS\n",
    "                    select arr_to_input_20(output_0) AS comments_embed from \n",
    "                        ML.PREDICT(MODEL trendspotting.swivel_text_embed,(\n",
    "                      SELECT date, geo_name, term AS sentences, volume\n",
    "                      FROM `{source_table}`\n",
    "                      WHERE date >= '{train_st}'\n",
    "                      and category_id = {subcat_id}\n",
    "                      ))\n",
    "    \"\"\"\n",
    "    #tell if the scored topic tables exist\n",
    "    \n",
    "    sct_exists_task = components.if_tbl_exists(label_table, vertex_project)\n",
    "    with kfp.v2.dsl.Condition(sct_exists_task.output==\"True\"):\n",
    "        ### if labled data exists, we will create a model and auto cluster each category\n",
    "        \n",
    "        train_model_op = components.train_classification_model(\n",
    "              target_table = scored_classification_table,\n",
    "              source_table = fix_embed.outputs['term_train_table'],\n",
    "              label_table = label_table,\n",
    "              train_table = classification_train_table,\n",
    "              classification_model_name = classification_model_name,\n",
    "              project_id = vertex_project,\n",
    "              classification_budget_hours = classification_model_budget\n",
    "            ) \n",
    "        \n",
    "        auto_cluster_op = components.auto_cluster(\n",
    "            cluster_min = auto_max_cluster,\n",
    "            cluster_max = auto_min_cluster,\n",
    "            labels = label_list,\n",
    "            cluster_train_table = auto_cluster_train_table,\n",
    "            classified_terms_table = train_model_op.output,\n",
    "            target_table = auto_cluster_target_table,\n",
    "            project_id = vertex_project\n",
    "            )# -> NamedTuple('Outputs', [('target_table', str)]):\n",
    "        \n",
    "        aggregate_cluster_op = components.aggregate_clusters(\n",
    "            source_table = cluster_table,\n",
    "            category_table = auto_cluster_op.output,\n",
    "            target_table = cluster_table_agg,\n",
    "            train_st = train_st,\n",
    "            train_end = train_end,\n",
    "            valid_st = valid_st,\n",
    "            valid_end = valid_end,\n",
    "            model_name = model_name,\n",
    "            )# -> Name\n",
    "        #create training ds in vertex\n",
    "        time_series_dataset_create_op_high_level = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "            display_name=ds_display_name_cluster, \n",
    "            bq_source=aggregate_cluster_op.outputs['term_cluster_agg_table'],\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "        )\n",
    "\n",
    "        term_forecasting_op = gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "            display_name=f'train-cluster-forecast-futurama',\n",
    "            model_display_name='cluster-forecast-futurama',\n",
    "            dataset=time_series_dataset_create_op_high_level.outputs['dataset'],\n",
    "            context_window=context_window,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            budget_milli_node_hours=budget_milli_node_hours_cluster,\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "            export_evaluated_data_items=True,\n",
    "            export_evaluated_data_items_override_destination=True,\n",
    "            target_column='volume',\n",
    "            time_column='date',\n",
    "            time_series_identifier_column='series_id',\n",
    "            time_series_attribute_columns=['topic_id', 'category', 'comments_embed_p1', 'comments_embed_p2', 'comments_embed_p3', 'comments_embed_p4', 'comments_embed_p5', 'comments_embed_p6',\n",
    "                                          'comments_embed_p7', 'comments_embed_p8', 'comments_embed_p9', 'comments_embed_p10', 'comments_embed_p11', 'comments_embed_p12',\n",
    "                                          'comments_embed_p13', 'comments_embed_p14', 'comments_embed_p15', 'comments_embed_p16', 'comments_embed_p17', 'comments_embed_p18', \n",
    "                                          'comments_embed_p19', 'comments_embed_p20'],\n",
    "            unavailable_at_forecast_columns=['volume'],\n",
    "            available_at_forecast_columns=['date'],\n",
    "            data_granularity_unit='week',\n",
    "            data_granularity_count=1,\n",
    "            predefined_split_column_name= 'split_col', \n",
    "            optimization_objective='minimize-rmse',\n",
    "            column_transformations=components.COLUMN_TRANSFORMS_CLUSTER,\n",
    "            export_evaluated_data_items_bigquery_destination_uri = target_cluster_forecast_table, # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "        )\n",
    "\n",
    "    with kfp.v2.dsl.Condition(sct_exists_task.output==\"False\"):\n",
    "        train_k_means_op = BigqueryCreateModelJobOp(project=PROJECT_ID,\n",
    "                                               location='US',\n",
    "                                               query=model_train_sql\n",
    "                                               )\n",
    "        create_cluster_terms_op = components.nlp_featurize_and_cluster(\n",
    "            source_table = source_table,\n",
    "            target_table = cluster_table,\n",
    "            train_st = train_st,\n",
    "            train_end = train_end,\n",
    "            subcat_id = subcat_id,\n",
    "            model_name = model_name\n",
    "            ).after(train_k_means_op)# -> NamedTuple('Outputs', [('term_cluster_table', str)]):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f371ada-8738-4f68-9a84-a81aa14da8ca",
   "metadata": {},
   "source": [
    "## todo - to get explainations\n",
    "drop predictions on train , use `google_cloud_pipeline_components.aiplatform.ModelBatchPredictOp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d9ec46-05d3-4dba-a935-c58026a5e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='trendspotting.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f6a37-e374-4570-b462-fd9d6b93691f",
   "metadata": {},
   "source": [
    "### Set parameters for pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a112290-eb83-48b6-8006-fdd244a02679",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'cpg-cdp' # <--- TODO: If not set\n",
    "LOCATION = 'us-central1' # <--- TODO: If not set\n",
    "SERVICE_ACCOUNT = 'vertex-pipelines@cpg-cdp.iam.gserviceaccount.com' , # <--- TODO: Change This if needed\n",
    "N_CLUSTERS = 20\n",
    "K_MEANS_MODEL_NAME = f\"cpg-cdp.trendspotting.trendspotting_{N_CLUSTERS}_rmse_{SUFFIX}\"\n",
    "\n",
    "MODEL_NAME = f'cpg-cdp.trendspotting.{SUFFIX}_kmeans_{N_CLUSTERS}'\n",
    "# BQ dataset for source data source\n",
    "SOURCE_DATA = 'futurama_weekly'\n",
    "TOP_N_RESULTS = 500\n",
    "# TODO: Forecasting Configuration:\n",
    "HISTORY_WINDOW_n = 52 #  {type: 'integer'} # context_window\n",
    "FORECAST_HORIZON = 52 #  {type: 'integer'} \n",
    "BUDGET_MILLI_NODE_HOURS = 20000\n",
    "BUDGET_MILLI_NODE_HOURS_CLUSTER = 20000\n",
    "BUDGET_HOURS_CLASSIFICATION = 2\n",
    "\n",
    "categories = ['Hair Straighteners and Relaxers',\n",
    " 'Near me',\n",
    " 'Scalp/Anti-Dandruff Products',\n",
    " 'Hair Dyes & Coloring',\n",
    " 'Damaged Hair',\n",
    " 'Hair Styling',\n",
    " 'Hair Loss Products',\n",
    " 'Lice',\n",
    " 'Shampoos & Conditioners']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc9f7ac-9937-4f1a-baa4-643d6e688648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b46db-4137-4ab6-8c68-b9b1d4153b55",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Follow the link to see the exectution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87946ac5-3fde-4251-b5bd-8e26e184832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/939655404703/locations/us-central1/pipelineJobs/v4-png-hair-22-trendspotting-pipeline-v4-20220907153554\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/939655404703/locations/us-central1/pipelineJobs/v4-png-hair-22-trendspotting-pipeline-v4-20220907153554')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/v4-png-hair-22-trendspotting-pipeline-v4-20220907153554?project=939655404703\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "PIPELINE_PARAMETERS = {\n",
    "    'subcat_id': 10055, #hair care for just this run\n",
    "    'vertex_project': PROJECT_ID,\n",
    "    'location': LOCATION,\n",
    "    'version': VERSION,\n",
    "    'label_table': f'trendspotting.labels_jw_pl_{SUFFIX}',\n",
    "    'scored_classification_table': f'cpg-cdp.ETL_trendspotting.classified_terms_bqml_aml_pl_{SUFFIX}',\n",
    "    'fix_embed_target': f'cpg-cdp.trendspotting.ETL_futurama_weekly_embed_aml_pl_{SUFFIX}',\n",
    "    'classification_train_table': f'trendspotting.ETL_labeled_distinct_training_jw_pl_{SUFFIX}',\n",
    "    'classification_model_name': f'trendspotting.bqml_distinct_pl_{SUFFIX}',\n",
    "    'classification_model_budget': BUDGET_HOURS_CLASSIFICATION,\n",
    "    'auto_min_cluster': 2,\n",
    "    'auto_max_cluster': 10,\n",
    "    'auto_cluster_train_table': f'trendspotting.cat_clus_train_{SUFFIX}',\n",
    "    'auto_cluster_target_table': f'trendspotting.full_cat_clus_{SUFFIX}',\n",
    "    'label_list' : categories,\n",
    "    'train_st': '2021-05-30',\n",
    "    'train_end': '2021-10-10',\n",
    "    'valid_st': '2021-10-17',\n",
    "    'valid_end': '2021-12-26',\n",
    "    'predict_on_dt': '2022-01-02',\n",
    "    'six_month_dt': '2022-07-17',\n",
    "    'context_window': HISTORY_WINDOW_n,\n",
    "    'forecast_horizon': FORECAST_HORIZON,\n",
    "    'budget_milli_node_hours': BUDGET_MILLI_NODE_HOURS,\n",
    "    'ds_display_name_terms': f'futurama-term-forecasts-{SUFFIX}',\n",
    "    'ds_display_name_cluster': f'futurama-clusters-{SUFFIX}',\n",
    "    'k_means_name': K_MEANS_MODEL_NAME,\n",
    "    'n_clusters': N_CLUSTERS,\n",
    "    'top_n_results': TOP_N_RESULTS,\n",
    "    'cluster_table_agg': f\"cpg-cdp.trendspotting.ETL_futurama_weekly_embed_cluster_agg_100_{SUFFIX}\",\n",
    "    'cluster_table': f\"cpg-cdp.trendspotting.ETL_futurama_weekly_embed_cluster_100_{SUFFIX}\",\n",
    "    'override' : 'false',\n",
    "    'target_term_forecast_table' : f'bq://cpg-cdp.trendspotting.ETL_predict_c52_p52_embed_pl_{SUFFIX}',\n",
    "    'source_table' : f'cpg-cdp.trendspotting.futurama_weekly', #FIX -TODO\n",
    "    'target_term_forecast_table': f'cpg-cdp.trendspotting.ETL_predict_{SUFFIX}',\n",
    "    'target_cluster_forecast_table': f'cpg-cdp.trendspotting.predict_cluster_{SUFFIX}',\n",
    "    'top_movers_target_table': f'cpg-cdp.trendspotting.top_movers_pl_{SUFFIX}',\n",
    "    'budget_milli_node_hours_cluster': BUDGET_MILLI_NODE_HOURS_CLUSTER,\n",
    "    'model_name': MODEL_NAME\n",
    "    }\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = f'trendspotting_{PIPELINE_PARAMETERS[\"subcat_id\"]}_{SUFFIX}',\n",
    "                             template_path = 'trendspotting.json',\n",
    "                             pipeline_root = PIPELINES_FILEPATH,\n",
    "                             parameter_values = PIPELINE_PARAMETERS,\n",
    "                             project = PROJECT_ID,\n",
    "                             location = LOCATION,\n",
    "                              enable_caching=True)\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc706d8-c491-4bfe-b979-0eff321957aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4828d-4ecd-4228-ab0d-b0e31037a001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c453b-aea3-40db-8bd5-6c67a31a0c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
