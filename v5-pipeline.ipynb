{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e4af0c-18f3-43f4-b418-17b48f156ef8",
   "metadata": {},
   "source": [
    "# Trendspotting POC\n",
    "\n",
    "Goal of this notebook is to\n",
    "* Load signals data into a managed vertex dataset for time series forecasting\n",
    "* Create a forecast prediction model for each term, geo and category combination\n",
    "* Project the forecasts on a holdout set of data to assess performance and trends\n",
    "* Clean up results of test predictions\n",
    "* Cluster test predictions\n",
    "* Create dashboard for backtesting\n",
    "\n",
    "End users would take this parameterized pipeline to produce futurama backtests using clustering and forecasting\n",
    "\n",
    "[Source Control Link](https://source.cloud.google.com/cpg-cdp/trendspotting/+/master:pipeline_train.ipynb)\n",
    "\n",
    "When run - the piepline will look something like this:\n",
    "\n",
    "![pipeline example](img/pipeline_example.png)\n",
    "\n",
    "[Link to pipeline](https://pantheon.corp.google.com/vertex-ai/locations/us-central1/pipelines/runs/report-pipe-trendspotting-pipeline-20220309212043?authuser=0&project=cpg-cdp)\n",
    "## Install packages, create bucket (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d303d30-5821-4179-a13d-3c49455e72ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components==1.0.5\n",
    "# # !git clone https://github.com/kubeflow/pipelines.git\n",
    "# # !pip install pipelines/components/google-cloud/.\n",
    "# !pip install google-cloud-aiplatform\n",
    "# from google_cloud_pipeline_components.v1.bigquery import BigqueryCreateModelJobOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15945c-b978-45c4-9ecc-87e17c4097b6",
   "metadata": {},
   "source": [
    "### Import libs and types for KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7682f283-85c0-416b-9e24-5ae00e6b4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install protobuf==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b2a410-def8-43de-aa41-2ed9efe75d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, Union\n",
    "from IPython.display import clear_output\n",
    "from google_cloud_pipeline_components.v1.bigquery import BigqueryCreateModelJobOp\n",
    "\n",
    "from google import auth\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "from google_cloud_pipeline_components.experimental import bigquery as gcp_aip_bq\n",
    "\n",
    "import google.cloud.aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Input, Model\n",
    "from kfp.v2.components.types.type_utils import artifact_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a95fbc67-a62b-42f2-a756-a6e304bf582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'cpg-cdp'\n",
    "VERTEX_PROJECT = PROJECT_ID\n",
    "LOCATION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b675de8f-00f9-4d2f-9d5c-73e3d9e78683",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = 'gs://trendspotting-pipeline' # <--- TODO: CHANGE THIS; can be blank json file\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "    with open(PIPELINES_FILEPATH) as f:\n",
    "        PIPELINES = json.load(f)\n",
    "else:\n",
    "    PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "    with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "        json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d66fd7-22b7-4682-ad27-dfdb3bc1c551",
   "metadata": {},
   "source": [
    "### Pipeline meta-paramter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a112290-eb83-48b6-8006-fdd244a02679",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = 'vertex-pipelines@cpg-cdp.iam.gserviceaccount.com' , # <--- TODO: Change This if needed\n",
    "N_CLUSTERS = 20\n",
    "# BQ dataset for source data source\n",
    "TOP_N_RESULTS = 50\n",
    "# TODO: Forecasting Configuration:\n",
    "HISTORY_WINDOW_n = 52 #  {type: 'integer'} # context_window\n",
    "FORECAST_HORIZON = 52 #  {type: 'integer'} \n",
    "BUDGET_MILLI_NODE_HOURS = 20000\n",
    "BUDGET_MILLI_NODE_HOURS_CLUSTER = 1000\n",
    "BUDGET_HOURS_CLASSIFICATION = 1\n",
    "CATEGORY_ID = 10889\n",
    "\n",
    "SRC_TABLE_ID = 'cuisines_10889_thailand_2764'\n",
    "# SRC_TABLE_ID = 'cuisines_10889_malaysia_2458'\n",
    "# SRC_TABLE_ID = 'skincare_10047_unitedstates_2840'\n",
    "\n",
    "SRC_TABLE = f'{PROJECT_ID}.trends_data.{SRC_TABLE_ID}'\n",
    "K_MEANS_MODEL_NAME = f\"cpg-cdp.trendspotting.{SRC_TABLE_ID}_trendspotting_{N_CLUSTERS}\"\n",
    "MODEL_NAME = f'cpg-cdp.trendspotting.{SRC_TABLE_ID}_kmeans_{N_CLUSTERS}'\n",
    "\n",
    "VERSION = 'v7_1'\n",
    "\n",
    "TRAIN_ST = '2021-08-29'\n",
    "TRAIN_END = '2022-02-13'\n",
    "VALID_ST = '2022-02-13'\n",
    "VALID_END = '2022-02-27'\n",
    "PREDICT_ON_DT = '2022-03-06'\n",
    "SIX_MONTH_DT = '2022-10-02'\n",
    "\n",
    "categories = ['NA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46e5f2-b27f-4286-98e2-83f11962f217",
   "metadata": {},
   "source": [
    "### Component Code is organized in the `src/components` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb7cfff-57ef-4c97-94c0-396ad159c5ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34msrc\u001b[00m\n",
      "├── __init__.py\n",
      "├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   ├── __init__.cpython-37.pyc\n",
      "│   └── colspecs.cpython-37.pyc\n",
      "├── colspecs.py\n",
      "├── \u001b[01;34mcomponents\u001b[00m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── __init__.cpython-37.pyc\n",
      "│   │   └── components.cpython-37.pyc\n",
      "│   └── components.py\n",
      "├── \u001b[01;34mmodels\u001b[00m\n",
      "└── \u001b[01;34mthird_party\u001b[00m\n",
      "    └── \u001b[01;34mpython-path-specification\u001b[00m\n",
      "\n",
      "6 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cfd3a-e4b3-4de6-9486-901b6ab7583f",
   "metadata": {},
   "source": [
    "### Pipeline \n",
    "\n",
    "Uses custom components, also uses reusable vertex components for creating the training dataset and training the forecast models\n",
    "\n",
    "Notice the output for testing in BQ is set by `target_table`, assigned to `export_evaluated_data_items_bigquery_destination_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ec99f8a-ec3a-45d9-89c9-7f24e8d8f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components import components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61b9d896-c952-40b1-ba3b-4b14a973fcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_TAG = f'{SRC_TABLE_ID}-trendspotting-pipeline-{VERSION}' # <--- TODO; optionally name pipeline\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{PIPELINE_TAG}'.replace('_', '-'),\n",
    "        pipeline_root=PIPELINES_FILEPATH,\n",
    "\n",
    ")\n",
    "def pipeline(\n",
    "    vertex_project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    ds_display_name_terms: str,\n",
    "    ds_display_name_cluster: str,\n",
    "    label_table: str,\n",
    "    scored_classification_table: str,\n",
    "    classification_train_table: str,\n",
    "    classification_model_name: str,\n",
    "    classification_model_budget: int,\n",
    "    auto_cluster_train_table: str,\n",
    "    auto_min_cluster: int,\n",
    "    auto_max_cluster: int,\n",
    "    auto_cluster_target_table: str,\n",
    "    label_list: list,\n",
    "    train_st: str,\n",
    "    train_end: str,\n",
    "    valid_st: str,\n",
    "    valid_end: str,\n",
    "    predict_on_dt: str,\n",
    "    override: str,\n",
    "    fix_embed_target: str,\n",
    "    drop_embed_target: str,\n",
    "    k_means_name: str,\n",
    "    n_clusters: int,\n",
    "    top_n_results: int,\n",
    "    six_month_dt: str,\n",
    "    source_table: str ,\n",
    "    target_term_forecast_table: str ,\n",
    "    target_cluster_forecast_table: str ,\n",
    "    budget_milli_node_hours: int,\n",
    "    budget_milli_node_hours_cluster: int,\n",
    "    context_window: int,\n",
    "    forecast_horizon: int,\n",
    "    top_movers_target_table: str,\n",
    "    cluster_table_agg: str,\n",
    "    cluster_table: str,\n",
    "    subcat_id: int,\n",
    "    model_name: str,\n",
    "    cluster_table_agg_basic: str,\n",
    "    target_cluster_forecast_table_basic: str,\n",
    "    target_cluster_forecast_table_basic_partitioned: str,\n",
    "    sustained_riser_table: str,\n",
    "):\n",
    "    \n",
    "    embed_terms = (components.create_prediction_dataset_term_level(\n",
    "      target_table = f'{vertex_project}.trends_pipeline.{SRC_TABLE_ID}_ETL_futurama_weekly_embed_{VERSION}',\n",
    "      source_table_uri = source_table,\n",
    "      train_st = train_st,\n",
    "      train_end = train_end,\n",
    "      valid_st = valid_st,\n",
    "      valid_end = valid_end,\n",
    "      subcat_id = subcat_id,\n",
    "        ).set_display_name('Add embeddings and split data')\n",
    "        .set_caching_options(True)\n",
    "                  )\n",
    "    \n",
    "    fix_embed = (components.prep_forecast_term_level(\n",
    "        source_table = embed_terms.outputs['training_data_table_uri'],\n",
    "        target_table = fix_embed_target,\n",
    "        )\n",
    "        .set_display_name('Prep Data For Training')\n",
    "        .set_caching_options(True)\n",
    "                )\n",
    "    \n",
    "    # fix_embed2 = (components.prep_forecast_term_level_drop_embeddings(\n",
    "    #     source_table = fix_embed.outputs['term_train_table'],\n",
    "    #     target_table = drop_embed_target,\n",
    "    #     )\n",
    "    #     .set_display_name('Dropping embeddings for point level forecasting')\n",
    "    #     .set_caching_options(True)\n",
    "    #              )\n",
    "\n",
    "\n",
    "    time_series_dataset_create_op = (gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "        display_name=ds_display_name_terms, \n",
    "        bq_source=fix_embed.outputs['term_train_table'],\n",
    "        project=vertex_project,\n",
    "        location=location,\n",
    "        )\n",
    "        .set_display_name('Prep data for training')\n",
    "        .set_caching_options(True)\n",
    "                                    )\n",
    "    \n",
    "    term_forecasting_op = (gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "        display_name=f'train-point-forecast-futurama',\n",
    "        model_display_name='point-forecast-futurama',\n",
    "        dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "        context_window=context_window,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        budget_milli_node_hours=budget_milli_node_hours,\n",
    "        project=vertex_project,\n",
    "        location=location,\n",
    "        export_evaluated_data_items=True,\n",
    "        export_evaluated_data_items_override_destination=True,\n",
    "        target_column='score',\n",
    "        time_column='date',\n",
    "        time_series_identifier_column='series_id',\n",
    "        time_series_attribute_columns=['sentences', 'geo_id', 'emb1', \n",
    "                                       'emb2', 'emb3', 'emb4', 'emb5', \n",
    "                                       'emb6', 'emb7', 'emb8', 'emb9', \n",
    "                                       'emb10', 'emb11', 'emb12', 'emb13', \n",
    "                                       'emb14', 'emb15', 'emb16', 'emb17', \n",
    "                                       'emb18', 'emb19', 'emb20'],\n",
    "        unavailable_at_forecast_columns=['score'],\n",
    "        available_at_forecast_columns=['date'],\n",
    "        data_granularity_unit='week',\n",
    "        data_granularity_count=1,\n",
    "        predefined_split_column_name= 'split_col', \n",
    "        optimization_objective='minimize-rmse',\n",
    "        # column_transformations=components.COLUMN_TRANSFORMATIONS,\n",
    "        column_specs={\"date\": \"timestamp\", \n",
    "                      \"geo_id\": \"categorical\",\n",
    "                      \"score\": \"numeric\",\n",
    "                      \"sentences\": \"categorical\",\n",
    "                      \"emb1\": \"numeric\",\n",
    "                      \"emb2\": \"numeric\",\n",
    "                      \"emb3\": \"numeric\",\n",
    "                      \"emb4\": \"numeric\",\n",
    "                      \"emb5\": \"numeric\",\n",
    "                      \"emb6\": \"numeric\",\n",
    "                      \"emb7\": \"numeric\",\n",
    "                      \"emb8\": \"numeric\",\n",
    "                      \"emb9\": \"numeric\",\n",
    "                      \"emb10\": \"numeric\",\n",
    "                      \"emb11\": \"numeric\",\n",
    "                      \"emb12\": \"numeric\",\n",
    "                      \"emb13\": \"numeric\",\n",
    "                      \"emb14\": \"numeric\",\n",
    "                      \"emb15\": \"numeric\",\n",
    "                      \"emb16\": \"numeric\",\n",
    "                      \"emb17\": \"numeric\",\n",
    "                      \"emb18\": \"numeric\",\n",
    "                      \"emb19\": \"numeric\",\n",
    "                      \"emb20\": \"numeric\",\n",
    "                     },\n",
    "        export_evaluated_data_items_bigquery_destination_uri = target_term_forecast_table, # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "        )\n",
    "        .set_display_name('Forecast term-level')\n",
    "        .set_caching_options(True)\n",
    "                          )\n",
    "    \n",
    "    sustained_risers_data_op = (components.sustained_riser_report(source_table=target_term_forecast_table,\n",
    "                                                                target_table=sustained_riser_table,\n",
    "                                                                top_n=top_n_results,\n",
    "                                                                predicted_on_dt=predict_on_dt)\n",
    "                              .after(term_forecasting_op)\n",
    "                              .set_display_name('Create Sustained Riser Table')\n",
    "                              .set_caching_options(True)\n",
    "                             )\n",
    "    \n",
    "    top_movers_data_op = (components.create_top_mover_table(source_table = target_term_forecast_table,\n",
    "    target_table = top_movers_target_table,\n",
    "        predict_on_dt = predict_on_dt, \n",
    "        six_month_dt = six_month_dt,\n",
    "        trained_model = term_forecasting_op.outputs['model'],\n",
    "        top_n_results = top_n_results,\n",
    "        )\n",
    "        .set_display_name('Generate the top mover table')\n",
    "        .set_caching_options(True)\n",
    "                         )\n",
    "    \n",
    "    top_mover_post_process = (\n",
    "        components.alter_topmover_schema(top_movers_target_table)\n",
    "        .after(top_movers_data_op)\n",
    "        .set_caching_options(True)\n",
    "        .set_display_name(\"Adding descriptions to the output table\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #HIGH LEVEL REPORT PIPELINE STARTS HERE\n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    model_train_sql = components.get_model_train_sql(model_name, \n",
    "                                                     n_clusters, \n",
    "                                                     source_table, \n",
    "                                                     train_st, \n",
    "                                                     subcat_id)\n",
    "    #tell if the scored topic tables exist\n",
    "    \n",
    "    sct_exists_task = components.if_tbl_exists(label_table, vertex_project)\n",
    "    with kfp.v2.dsl.Condition(sct_exists_task.output==\"True\"):\n",
    "        ### if labled data exists, we will create a model and auto cluster each category\n",
    "        \n",
    "        train_model_op = (components.train_classification_model(\n",
    "              target_table = scored_classification_table,\n",
    "              source_table = fix_embed.outputs['term_train_table'],\n",
    "              label_table = label_table,\n",
    "              train_table = classification_train_table,\n",
    "              classification_model_name = classification_model_name,\n",
    "              project_id = vertex_project,\n",
    "              classification_budget_hours = classification_model_budget\n",
    "            )\n",
    "            .set_display_name('Train classification model on examples')\n",
    "            .set_caching_options(True) \n",
    "                         )\n",
    "        \n",
    "        auto_cluster_op = (components.auto_cluster(\n",
    "            cluster_min = auto_min_cluster,\n",
    "            cluster_max = auto_max_cluster,\n",
    "            labels = label_list,\n",
    "            cluster_train_table = auto_cluster_train_table,\n",
    "            classified_terms_table = train_model_op.output,\n",
    "            target_table = auto_cluster_target_table,\n",
    "            project_id = vertex_project\n",
    "            )\n",
    "            .set_display_name('Auto cluster each category')\n",
    "            .set_caching_options(True)\n",
    "                          )\n",
    "        \n",
    "        aggregate_cluster_op = (components.aggregate_clusters(\n",
    "            source_table = cluster_table,\n",
    "            category_table = auto_cluster_op.output,\n",
    "            target_table = cluster_table_agg,\n",
    "            train_st = train_st,\n",
    "            train_end = train_end,\n",
    "            valid_st = valid_st,\n",
    "            valid_end = valid_end,\n",
    "            model_name = model_name,\n",
    "            )\n",
    "            .set_display_name('Aggregate category clusters')\n",
    "            .set_caching_options(True)\n",
    "                               )\n",
    "            \n",
    "        time_series_dataset_create_op_high_level = (gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "            display_name=ds_display_name_cluster, \n",
    "            bq_source=aggregate_cluster_op.outputs['term_cluster_agg_table'],\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "            )\n",
    "            .set_display_name('Forecast term-level')\n",
    "            .set_caching_options(True)\n",
    "                                                   )\n",
    "\n",
    "\n",
    "        term_forecasting_op = (gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "            display_name=f'train-cluster-forecast-futurama',\n",
    "            model_display_name='cluster-forecast-futurama',\n",
    "            dataset=time_series_dataset_create_op_high_level.outputs['dataset'],\n",
    "            context_window=context_window,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            budget_milli_node_hours=budget_milli_node_hours_cluster,\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "            export_evaluated_data_items=True,\n",
    "            export_evaluated_data_items_override_destination=True,\n",
    "            target_column='score',\n",
    "            time_column='date',\n",
    "            time_series_identifier_column='series_id',\n",
    "            time_series_attribute_columns=['topic_id', 'category', 'comments_embed_p1', 'comments_embed_p2', 'comments_embed_p3', 'comments_embed_p4', 'comments_embed_p5', 'comments_embed_p6',\n",
    "                                          'comments_embed_p7', 'comments_embed_p8', 'comments_embed_p9', 'comments_embed_p10', 'comments_embed_p11', 'comments_embed_p12',\n",
    "                                          'comments_embed_p13', 'comments_embed_p14', 'comments_embed_p15', 'comments_embed_p16', 'comments_embed_p17', 'comments_embed_p18', \n",
    "                                          'comments_embed_p19', 'comments_embed_p20'],\n",
    "            unavailable_at_forecast_columns=['score'],\n",
    "            available_at_forecast_columns=['date'],\n",
    "            data_granularity_unit='week',\n",
    "            data_granularity_count=1,\n",
    "            predefined_split_column_name= 'split_col', \n",
    "            optimization_objective='minimize-rmse',\n",
    "            column_transformations=components.COLUMN_TRANSFORMS_CLUSTER,\n",
    "            export_evaluated_data_items_bigquery_destination_uri = target_cluster_forecast_table, # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "            )\n",
    "            .set_display_name('Forecast category clusters')\n",
    "            .set_caching_options(True)\n",
    "                              )\n",
    "\n",
    "    with kfp.v2.dsl.Condition(sct_exists_task.output==\"False\"):\n",
    "        train_k_means_op = (BigqueryCreateModelJobOp(project=PROJECT_ID,\n",
    "                                               location='US',\n",
    "                                               query=model_train_sql\n",
    "                                               )\n",
    "                                .set_display_name('Train k-means basic BQ Model')\n",
    "                                .set_caching_options(True)\n",
    "                           )\n",
    "                \n",
    "        create_cluster_terms_op = (components.nlp_featurize_and_cluster(\n",
    "            source_table = source_table,\n",
    "            target_table = cluster_table,\n",
    "            train_st = train_st,\n",
    "            train_end = train_end,\n",
    "            subcat_id = subcat_id,\n",
    "            model_name = model_name,\n",
    "            n_clusters = n_clusters\n",
    "            ).after(train_k_means_op\n",
    "            )\n",
    "            .set_display_name('Add NLP embeddings and cluster')\n",
    "            .set_caching_options(True)\n",
    "                                  )\n",
    "        \n",
    "        cluster_term_table_basic_post_processing = (components.alter_basic_cluster_term_table(cluster_table)\n",
    "                                                   .after(create_cluster_terms_op)\n",
    "                                                   .set_display_name(\"Altering table descriptions\")\n",
    "                                                   .set_caching_options(True)\n",
    "                                                   )\n",
    "        \n",
    "        aggregate_cluster_op = (components.aggregate_clusters_basic(\n",
    "            source_table = create_cluster_terms_op.outputs['term_cluster_table'],\n",
    "            target_table = cluster_table_agg_basic,\n",
    "            train_st = train_st,\n",
    "            train_end = train_end,\n",
    "            valid_st = valid_st,\n",
    "            valid_end = valid_end,\n",
    "            )\n",
    "            .set_display_name('Basic clustering (unsupervised)')\n",
    "            .set_caching_options(True)\n",
    "                               )\n",
    "        \n",
    "        time_series_dataset_create_op_high_level = (gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "            display_name=ds_display_name_cluster, \n",
    "            bq_source=aggregate_cluster_op.outputs['term_cluster_agg_table'],\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "            )\n",
    "            .set_display_name('Aggregate basic clusters')\n",
    "            .set_caching_options(True)\n",
    "                                                   )\n",
    "            \n",
    "        term_forecasting_op = (gcc_aip.AutoMLForecastingTrainingJobRunOp(\n",
    "            display_name=f'train-cluster-forecast-futurama',\n",
    "            model_display_name='cluster-forecast-futurama',\n",
    "            dataset=time_series_dataset_create_op_high_level.outputs['dataset'],\n",
    "            context_window=context_window,\n",
    "            forecast_horizon=forecast_horizon,\n",
    "            budget_milli_node_hours=budget_milli_node_hours_cluster,\n",
    "            project=vertex_project,\n",
    "            location=location,\n",
    "            export_evaluated_data_items=True,\n",
    "            export_evaluated_data_items_override_destination=True,\n",
    "            target_column='score',\n",
    "            time_column='date',\n",
    "            time_series_identifier_column='topic_id',\n",
    "            time_series_attribute_columns=['comments_embed_p1', 'comments_embed_p2', 'comments_embed_p3', 'comments_embed_p4', 'comments_embed_p5', 'comments_embed_p6',\n",
    "                                          'comments_embed_p7', 'comments_embed_p8', 'comments_embed_p9', 'comments_embed_p10', 'comments_embed_p11', 'comments_embed_p12',\n",
    "                                          'comments_embed_p13', 'comments_embed_p14', 'comments_embed_p15', 'comments_embed_p16', 'comments_embed_p17', 'comments_embed_p18', \n",
    "                                          'comments_embed_p19', 'comments_embed_p20'],\n",
    "            unavailable_at_forecast_columns=['score'],\n",
    "            available_at_forecast_columns=['date'],\n",
    "            data_granularity_unit='week',\n",
    "            data_granularity_count=1,\n",
    "            predefined_split_column_name= 'split_col', \n",
    "            optimization_objective='minimize-rmse',\n",
    "            column_transformations=components.COLUMN_TRANSFORMS_CLUSTER,\n",
    "            export_evaluated_data_items_bigquery_destination_uri = target_cluster_forecast_table_basic, # must be format:``bq://<project_id>:<dataset_id>:<table>``\\n\",\n",
    "            )\n",
    "            .set_display_name('Forecast basic clusters')\n",
    "            .set_caching_options(True)\n",
    "                              )\n",
    "        \n",
    "        cluster_forecast_fix_table_op = (components.create_partitioned_forecast_table(target_cluster_forecast_table_basic,\n",
    "                                                                                     target_cluster_forecast_table_basic_partitioned)\n",
    "                                         .after(term_forecasting_op)\n",
    "                                         .set_display_name('Creating final partitioned table')\n",
    "                                         .set_caching_options(True)\n",
    "                                        )\n",
    "        \n",
    "        cluster_forecast_table_post_process = (components.alter_basic_cluster_forecast_table(target_cluster_forecast_table_basic_partitioned)\n",
    "                                               .after(cluster_forecast_fix_table_op)\n",
    "                                               .set_display_name('Adding table descriptions')\n",
    "                                               .set_caching_options(True)\n",
    "                                              )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f371ada-8738-4f68-9a84-a81aa14da8ca",
   "metadata": {},
   "source": [
    "## todo - to get explainations\n",
    "drop predictions on train , use `google_cloud_pipeline_components.aiplatform.ModelBatchPredictOp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8d9ec46-05d3-4dba-a935-c58026a5e822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='trendspotting.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f6a37-e374-4570-b462-fd9d6b93691f",
   "metadata": {},
   "source": [
    "### Set parameters for pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc9f7ac-9937-4f1a-baa4-643d6e688648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b46db-4137-4ab6-8c68-b9b1d4153b55",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Follow the link to see the exectution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87946ac5-3fde-4251-b5bd-8e26e184832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/939655404703/locations/us-central1/pipelineJobs/cuisines-10889-thailand-2764-trendspotting-pipeline-v7-1-20230110163244\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/939655404703/locations/us-central1/pipelineJobs/cuisines-10889-thailand-2764-trendspotting-pipeline-v7-1-20230110163244')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/cuisines-10889-thailand-2764-trendspotting-pipeline-v7-1-20230110163244?project=939655404703\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "PIPELINE_PARAMETERS = {\n",
    "    'subcat_id': CATEGORY_ID, \n",
    "    'vertex_project': PROJECT_ID,\n",
    "    'location': LOCATION,\n",
    "    'version': VERSION,\n",
    "    'label_table': f'{PROJECT_ID}.trends_pipeline.labels_jw_pl_{VERSION}',\n",
    "    'scored_classification_table': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_classified_terms_bqml_aml_pl_{VERSION}',\n",
    "    'fix_embed_target': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_futurama_weekly_embed_aml_pl_{VERSION}',\n",
    "    'drop_embed_target': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_futurama_weekly_no_embed_aml_pl_{VERSION}',\n",
    "    'classification_train_table': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_labeled_distinct_training_jw_pl_{VERSION}',\n",
    "    'classification_model_name': f'trends_pipeline.{SRC_TABLE_ID}_bqml_distinct_pl_{VERSION}',\n",
    "    'classification_model_budget': BUDGET_HOURS_CLASSIFICATION,\n",
    "    'auto_min_cluster': 2,\n",
    "    'auto_max_cluster': 9,\n",
    "    'auto_cluster_train_table': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_cat_clus_train_{VERSION}',\n",
    "    'auto_cluster_target_table': f'{PROJECT_ID}.trends_results.{SRC_TABLE_ID}_categoryclusters_{VERSION}',\n",
    "    'label_list' : categories,\n",
    "    'train_st': TRAIN_ST,\n",
    "    'train_end': TRAIN_END,\n",
    "    'valid_st': VALID_ST,\n",
    "    'valid_end': VALID_END,\n",
    "    'predict_on_dt': PREDICT_ON_DT,\n",
    "    'six_month_dt': SIX_MONTH_DT,\n",
    "    'context_window': HISTORY_WINDOW_n,\n",
    "    'forecast_horizon': FORECAST_HORIZON,\n",
    "    'budget_milli_node_hours': BUDGET_MILLI_NODE_HOURS,\n",
    "    'ds_display_name_terms': f'{SRC_TABLE_ID}-futurama-term-forecasts-{VERSION}',\n",
    "    'ds_display_name_cluster': f'{SRC_TABLE_ID}-futurama-clusters-{VERSION}',\n",
    "    'k_means_name': K_MEANS_MODEL_NAME,\n",
    "    'n_clusters': N_CLUSTERS,\n",
    "    'top_n_results': TOP_N_RESULTS,\n",
    "    'cluster_table_agg': f\"{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_futurama_weekly_embed_cluster_agg_{N_CLUSTERS}_{VERSION}\",\n",
    "    'target_cluster_forecast_table_basic': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_predictions_cluster_basic_{N_CLUSTERS}_{VERSION}',\n",
    "    'target_cluster_forecast_table_basic_partitioned': f'{PROJECT_ID}.trends_results.{SRC_TABLE_ID}_predictions_cluster_basic_{N_CLUSTERS}_{VERSION}',\n",
    "    'cluster_table': f\"{PROJECT_ID}.trends_results.{SRC_TABLE_ID}_categoryclusters_basic_{N_CLUSTERS}_{VERSION}\",\n",
    "    'cluster_table_agg_basic': f\"{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_futurama_weekly_embed_cluster_agg_basic_{N_CLUSTERS}_{VERSION}\",\n",
    "    'override' : 'false',\n",
    "    'source_table' : SRC_TABLE, \n",
    "    'target_term_forecast_table': f'{PROJECT_ID}.trends_pipeline.{SRC_TABLE_ID}_ETL_predict_{VERSION}',\n",
    "    'target_cluster_forecast_table': f'{PROJECT_ID}.trends_results.{SRC_TABLE_ID}_predictions_{VERSION}',\n",
    "    'top_movers_target_table': f'{PROJECT_ID}.trends_results.{SRC_TABLE_ID}_topmovers_{VERSION}',\n",
    "    'budget_milli_node_hours_cluster': BUDGET_MILLI_NODE_HOURS_CLUSTER,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'sustained_riser_table': f'{PROJECT_ID}.trends_results.{SRC_TABLE_ID}_sustained_risers_{VERSION}',\n",
    "    }\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = f'trendspotting_{PIPELINE_PARAMETERS[\"subcat_id\"]}_{VERSION}',\n",
    "                             template_path = 'trendspotting.json',\n",
    "                             pipeline_root = PIPELINES_FILEPATH,\n",
    "                             parameter_values = PIPELINE_PARAMETERS,\n",
    "                             project = PROJECT_ID,\n",
    "                             location = LOCATION,\n",
    "                             enable_caching=False)\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd9b3e-af0d-482c-8a2c-9194f9d0446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45fc3b-3b0a-43f9-ae59-9a44f276c82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
